{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 10000,
  "global_step": 251717,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003972715390696695,
      "grad_norm": 2.164712905883789,
      "learning_rate": 4.98015628662347e-05,
      "loss": 4.478,
      "step": 1000
    },
    {
      "epoch": 0.00794543078139339,
      "grad_norm": 2.4270002841949463,
      "learning_rate": 4.960292709669987e-05,
      "loss": 3.581,
      "step": 2000
    },
    {
      "epoch": 0.011918146172090084,
      "grad_norm": 2.607172966003418,
      "learning_rate": 4.940429132716503e-05,
      "loss": 3.2732,
      "step": 3000
    },
    {
      "epoch": 0.01589086156278678,
      "grad_norm": 2.7614495754241943,
      "learning_rate": 4.92056555576302e-05,
      "loss": 3.0687,
      "step": 4000
    },
    {
      "epoch": 0.019863576953483477,
      "grad_norm": 2.542607307434082,
      "learning_rate": 4.9007019788095364e-05,
      "loss": 2.9192,
      "step": 5000
    },
    {
      "epoch": 0.02383629234418017,
      "grad_norm": 2.6272027492523193,
      "learning_rate": 4.8808384018560526e-05,
      "loss": 2.814,
      "step": 6000
    },
    {
      "epoch": 0.027809007734876864,
      "grad_norm": 2.7858917713165283,
      "learning_rate": 4.8609748249025695e-05,
      "loss": 2.7143,
      "step": 7000
    },
    {
      "epoch": 0.03178172312557356,
      "grad_norm": 2.5599684715270996,
      "learning_rate": 4.841111247949086e-05,
      "loss": 2.6334,
      "step": 8000
    },
    {
      "epoch": 0.035754438516270255,
      "grad_norm": 2.7760322093963623,
      "learning_rate": 4.821247670995602e-05,
      "loss": 2.5521,
      "step": 9000
    },
    {
      "epoch": 0.039727153906966954,
      "grad_norm": 2.7707674503326416,
      "learning_rate": 4.801384094042119e-05,
      "loss": 2.5108,
      "step": 10000
    },
    {
      "epoch": 0.043699869297663646,
      "grad_norm": 2.6452953815460205,
      "learning_rate": 4.781520517088636e-05,
      "loss": 2.4447,
      "step": 11000
    },
    {
      "epoch": 0.04767258468836034,
      "grad_norm": 2.8913161754608154,
      "learning_rate": 4.761656940135152e-05,
      "loss": 2.3996,
      "step": 12000
    },
    {
      "epoch": 0.05164530007905704,
      "grad_norm": 2.6795382499694824,
      "learning_rate": 4.7417933631816684e-05,
      "loss": 2.3599,
      "step": 13000
    },
    {
      "epoch": 0.05561801546975373,
      "grad_norm": 2.906987190246582,
      "learning_rate": 4.721929786228185e-05,
      "loss": 2.3323,
      "step": 14000
    },
    {
      "epoch": 0.05959073086045043,
      "grad_norm": 2.741863965988159,
      "learning_rate": 4.7020662092747016e-05,
      "loss": 2.2926,
      "step": 15000
    },
    {
      "epoch": 0.06356344625114713,
      "grad_norm": 2.919097661972046,
      "learning_rate": 4.682202632321218e-05,
      "loss": 2.2638,
      "step": 16000
    },
    {
      "epoch": 0.06753616164184381,
      "grad_norm": 2.933354377746582,
      "learning_rate": 4.662339055367735e-05,
      "loss": 2.2293,
      "step": 17000
    },
    {
      "epoch": 0.07150887703254051,
      "grad_norm": 2.7936394214630127,
      "learning_rate": 4.642475478414251e-05,
      "loss": 2.2176,
      "step": 18000
    },
    {
      "epoch": 0.07548159242323721,
      "grad_norm": 2.6000733375549316,
      "learning_rate": 4.622611901460768e-05,
      "loss": 2.185,
      "step": 19000
    },
    {
      "epoch": 0.07945430781393391,
      "grad_norm": 2.8521530628204346,
      "learning_rate": 4.602748324507284e-05,
      "loss": 2.1685,
      "step": 20000
    },
    {
      "epoch": 0.0834270232046306,
      "grad_norm": 2.5925493240356445,
      "learning_rate": 4.5828847475538004e-05,
      "loss": 2.145,
      "step": 21000
    },
    {
      "epoch": 0.08739973859532729,
      "grad_norm": 2.833228588104248,
      "learning_rate": 4.5630211706003174e-05,
      "loss": 2.1332,
      "step": 22000
    },
    {
      "epoch": 0.09137245398602399,
      "grad_norm": 2.8549258708953857,
      "learning_rate": 4.543157593646834e-05,
      "loss": 2.1181,
      "step": 23000
    },
    {
      "epoch": 0.09534516937672068,
      "grad_norm": 2.690160036087036,
      "learning_rate": 4.52329401669335e-05,
      "loss": 2.0927,
      "step": 24000
    },
    {
      "epoch": 0.09931788476741737,
      "grad_norm": 2.7442400455474854,
      "learning_rate": 4.503430439739867e-05,
      "loss": 2.0722,
      "step": 25000
    },
    {
      "epoch": 0.10329060015811407,
      "grad_norm": 2.4374828338623047,
      "learning_rate": 4.483566862786384e-05,
      "loss": 2.0635,
      "step": 26000
    },
    {
      "epoch": 0.10726331554881077,
      "grad_norm": 2.634885787963867,
      "learning_rate": 4.4637032858329e-05,
      "loss": 2.0433,
      "step": 27000
    },
    {
      "epoch": 0.11123603093950746,
      "grad_norm": 2.853858709335327,
      "learning_rate": 4.443839708879416e-05,
      "loss": 2.0397,
      "step": 28000
    },
    {
      "epoch": 0.11520874633020416,
      "grad_norm": 2.6496353149414062,
      "learning_rate": 4.4239761319259325e-05,
      "loss": 2.0345,
      "step": 29000
    },
    {
      "epoch": 0.11918146172090086,
      "grad_norm": 2.5712647438049316,
      "learning_rate": 4.4041125549724494e-05,
      "loss": 2.0121,
      "step": 30000
    },
    {
      "epoch": 0.12315417711159755,
      "grad_norm": 2.802989959716797,
      "learning_rate": 4.384248978018966e-05,
      "loss": 1.9976,
      "step": 31000
    },
    {
      "epoch": 0.12712689250229425,
      "grad_norm": 2.989732503890991,
      "learning_rate": 4.364385401065482e-05,
      "loss": 1.9881,
      "step": 32000
    },
    {
      "epoch": 0.13109960789299094,
      "grad_norm": 2.5811452865600586,
      "learning_rate": 4.344521824111999e-05,
      "loss": 1.9777,
      "step": 33000
    },
    {
      "epoch": 0.13507232328368762,
      "grad_norm": 2.6718809604644775,
      "learning_rate": 4.324658247158516e-05,
      "loss": 1.9751,
      "step": 34000
    },
    {
      "epoch": 0.13904503867438434,
      "grad_norm": 2.621722459793091,
      "learning_rate": 4.304794670205032e-05,
      "loss": 1.9719,
      "step": 35000
    },
    {
      "epoch": 0.14301775406508102,
      "grad_norm": 2.590946674346924,
      "learning_rate": 4.284931093251548e-05,
      "loss": 1.956,
      "step": 36000
    },
    {
      "epoch": 0.1469904694557777,
      "grad_norm": 2.9182932376861572,
      "learning_rate": 4.265067516298065e-05,
      "loss": 1.9476,
      "step": 37000
    },
    {
      "epoch": 0.15096318484647442,
      "grad_norm": 2.822206974029541,
      "learning_rate": 4.2452039393445814e-05,
      "loss": 1.9419,
      "step": 38000
    },
    {
      "epoch": 0.1549359002371711,
      "grad_norm": 2.584728479385376,
      "learning_rate": 4.2253403623910983e-05,
      "loss": 1.9352,
      "step": 39000
    },
    {
      "epoch": 0.15890861562786782,
      "grad_norm": 2.7515416145324707,
      "learning_rate": 4.2054767854376146e-05,
      "loss": 1.9205,
      "step": 40000
    },
    {
      "epoch": 0.1628813310185645,
      "grad_norm": 2.8406662940979004,
      "learning_rate": 4.185613208484131e-05,
      "loss": 1.9183,
      "step": 41000
    },
    {
      "epoch": 0.1668540464092612,
      "grad_norm": 2.512110710144043,
      "learning_rate": 4.165749631530648e-05,
      "loss": 1.9057,
      "step": 42000
    },
    {
      "epoch": 0.1708267617999579,
      "grad_norm": 2.5433590412139893,
      "learning_rate": 4.145886054577165e-05,
      "loss": 1.8984,
      "step": 43000
    },
    {
      "epoch": 0.17479947719065458,
      "grad_norm": 2.332103967666626,
      "learning_rate": 4.12602247762368e-05,
      "loss": 1.8976,
      "step": 44000
    },
    {
      "epoch": 0.17877219258135127,
      "grad_norm": 2.7516870498657227,
      "learning_rate": 4.106158900670197e-05,
      "loss": 1.8835,
      "step": 45000
    },
    {
      "epoch": 0.18274490797204798,
      "grad_norm": 2.4930427074432373,
      "learning_rate": 4.086295323716714e-05,
      "loss": 1.8781,
      "step": 46000
    },
    {
      "epoch": 0.18671762336274467,
      "grad_norm": 2.7686946392059326,
      "learning_rate": 4.0664317467632304e-05,
      "loss": 1.8822,
      "step": 47000
    },
    {
      "epoch": 0.19069033875344135,
      "grad_norm": 2.3344414234161377,
      "learning_rate": 4.0465681698097466e-05,
      "loss": 1.8717,
      "step": 48000
    },
    {
      "epoch": 0.19466305414413806,
      "grad_norm": 2.608945608139038,
      "learning_rate": 4.0267045928562635e-05,
      "loss": 1.8737,
      "step": 49000
    },
    {
      "epoch": 0.19863576953483475,
      "grad_norm": 2.5947704315185547,
      "learning_rate": 4.00684101590278e-05,
      "loss": 1.8592,
      "step": 50000
    },
    {
      "epoch": 0.20260848492553146,
      "grad_norm": 2.6668975353240967,
      "learning_rate": 3.986977438949297e-05,
      "loss": 1.8589,
      "step": 51000
    },
    {
      "epoch": 0.20658120031622815,
      "grad_norm": 2.222278118133545,
      "learning_rate": 3.967113861995813e-05,
      "loss": 1.8523,
      "step": 52000
    },
    {
      "epoch": 0.21055391570692483,
      "grad_norm": 2.482802152633667,
      "learning_rate": 3.947250285042329e-05,
      "loss": 1.8415,
      "step": 53000
    },
    {
      "epoch": 0.21452663109762155,
      "grad_norm": 2.536522626876831,
      "learning_rate": 3.927386708088846e-05,
      "loss": 1.8352,
      "step": 54000
    },
    {
      "epoch": 0.21849934648831823,
      "grad_norm": 2.4499528408050537,
      "learning_rate": 3.9075231311353624e-05,
      "loss": 1.8421,
      "step": 55000
    },
    {
      "epoch": 0.22247206187901492,
      "grad_norm": 2.7792720794677734,
      "learning_rate": 3.8876595541818787e-05,
      "loss": 1.8385,
      "step": 56000
    },
    {
      "epoch": 0.22644477726971163,
      "grad_norm": 2.690570116043091,
      "learning_rate": 3.8677959772283956e-05,
      "loss": 1.8191,
      "step": 57000
    },
    {
      "epoch": 0.2304174926604083,
      "grad_norm": 2.3738725185394287,
      "learning_rate": 3.8479324002749125e-05,
      "loss": 1.8273,
      "step": 58000
    },
    {
      "epoch": 0.234390208051105,
      "grad_norm": 2.5589160919189453,
      "learning_rate": 3.828068823321429e-05,
      "loss": 1.8212,
      "step": 59000
    },
    {
      "epoch": 0.2383629234418017,
      "grad_norm": 2.623065233230591,
      "learning_rate": 3.808205246367945e-05,
      "loss": 1.8165,
      "step": 60000
    },
    {
      "epoch": 0.2423356388324984,
      "grad_norm": 2.4983668327331543,
      "learning_rate": 3.788341669414462e-05,
      "loss": 1.8113,
      "step": 61000
    },
    {
      "epoch": 0.2463083542231951,
      "grad_norm": 2.624427318572998,
      "learning_rate": 3.768478092460978e-05,
      "loss": 1.8076,
      "step": 62000
    },
    {
      "epoch": 0.2502810696138918,
      "grad_norm": 2.487750291824341,
      "learning_rate": 3.7486145155074944e-05,
      "loss": 1.8004,
      "step": 63000
    },
    {
      "epoch": 0.2542537850045885,
      "grad_norm": 2.4467458724975586,
      "learning_rate": 3.7287509385540114e-05,
      "loss": 1.7962,
      "step": 64000
    },
    {
      "epoch": 0.25822650039528516,
      "grad_norm": 2.6507039070129395,
      "learning_rate": 3.7088873616005276e-05,
      "loss": 1.8001,
      "step": 65000
    },
    {
      "epoch": 0.2621992157859819,
      "grad_norm": 2.5640158653259277,
      "learning_rate": 3.6890237846470445e-05,
      "loss": 1.7828,
      "step": 66000
    },
    {
      "epoch": 0.2661719311766786,
      "grad_norm": 2.5303077697753906,
      "learning_rate": 3.669160207693561e-05,
      "loss": 1.7858,
      "step": 67000
    },
    {
      "epoch": 0.27014464656737525,
      "grad_norm": 2.897005081176758,
      "learning_rate": 3.649296630740077e-05,
      "loss": 1.786,
      "step": 68000
    },
    {
      "epoch": 0.27411736195807196,
      "grad_norm": 2.691469192504883,
      "learning_rate": 3.629433053786594e-05,
      "loss": 1.7826,
      "step": 69000
    },
    {
      "epoch": 0.27809007734876867,
      "grad_norm": 2.6537797451019287,
      "learning_rate": 3.609569476833111e-05,
      "loss": 1.7815,
      "step": 70000
    },
    {
      "epoch": 0.28206279273946533,
      "grad_norm": 2.3283817768096924,
      "learning_rate": 3.5897058998796265e-05,
      "loss": 1.7783,
      "step": 71000
    },
    {
      "epoch": 0.28603550813016204,
      "grad_norm": 2.2987277507781982,
      "learning_rate": 3.5698423229261434e-05,
      "loss": 1.7682,
      "step": 72000
    },
    {
      "epoch": 0.29000822352085875,
      "grad_norm": 2.3601133823394775,
      "learning_rate": 3.5499787459726596e-05,
      "loss": 1.7723,
      "step": 73000
    },
    {
      "epoch": 0.2939809389115554,
      "grad_norm": 2.7625038623809814,
      "learning_rate": 3.5301151690191766e-05,
      "loss": 1.7714,
      "step": 74000
    },
    {
      "epoch": 0.2979536543022521,
      "grad_norm": 2.5750930309295654,
      "learning_rate": 3.510251592065693e-05,
      "loss": 1.7602,
      "step": 75000
    },
    {
      "epoch": 0.30192636969294884,
      "grad_norm": 2.4280574321746826,
      "learning_rate": 3.490388015112209e-05,
      "loss": 1.7601,
      "step": 76000
    },
    {
      "epoch": 0.30589908508364555,
      "grad_norm": 2.5896129608154297,
      "learning_rate": 3.470524438158726e-05,
      "loss": 1.7498,
      "step": 77000
    },
    {
      "epoch": 0.3098718004743422,
      "grad_norm": 2.7005770206451416,
      "learning_rate": 3.450660861205243e-05,
      "loss": 1.7566,
      "step": 78000
    },
    {
      "epoch": 0.3138445158650389,
      "grad_norm": 2.308988332748413,
      "learning_rate": 3.430797284251759e-05,
      "loss": 1.7608,
      "step": 79000
    },
    {
      "epoch": 0.31781723125573563,
      "grad_norm": 2.6035730838775635,
      "learning_rate": 3.4109337072982754e-05,
      "loss": 1.7499,
      "step": 80000
    },
    {
      "epoch": 0.3217899466464323,
      "grad_norm": 2.3553097248077393,
      "learning_rate": 3.3910701303447923e-05,
      "loss": 1.7495,
      "step": 81000
    },
    {
      "epoch": 0.325762662037129,
      "grad_norm": 2.2965433597564697,
      "learning_rate": 3.3712065533913086e-05,
      "loss": 1.7497,
      "step": 82000
    },
    {
      "epoch": 0.3297353774278257,
      "grad_norm": 2.4057865142822266,
      "learning_rate": 3.351342976437825e-05,
      "loss": 1.7421,
      "step": 83000
    },
    {
      "epoch": 0.3337080928185224,
      "grad_norm": 2.468996286392212,
      "learning_rate": 3.331479399484342e-05,
      "loss": 1.7388,
      "step": 84000
    },
    {
      "epoch": 0.3376808082092191,
      "grad_norm": 2.562502145767212,
      "learning_rate": 3.311615822530858e-05,
      "loss": 1.7367,
      "step": 85000
    },
    {
      "epoch": 0.3416535235999158,
      "grad_norm": 2.340111017227173,
      "learning_rate": 3.291752245577375e-05,
      "loss": 1.7321,
      "step": 86000
    },
    {
      "epoch": 0.34562623899061246,
      "grad_norm": 2.3669376373291016,
      "learning_rate": 3.271888668623891e-05,
      "loss": 1.7321,
      "step": 87000
    },
    {
      "epoch": 0.34959895438130917,
      "grad_norm": 2.2999303340911865,
      "learning_rate": 3.2520250916704075e-05,
      "loss": 1.7309,
      "step": 88000
    },
    {
      "epoch": 0.3535716697720059,
      "grad_norm": 2.3535850048065186,
      "learning_rate": 3.2321615147169244e-05,
      "loss": 1.7191,
      "step": 89000
    },
    {
      "epoch": 0.35754438516270254,
      "grad_norm": 2.4328696727752686,
      "learning_rate": 3.212297937763441e-05,
      "loss": 1.721,
      "step": 90000
    },
    {
      "epoch": 0.36151710055339925,
      "grad_norm": 2.4742279052734375,
      "learning_rate": 3.192434360809957e-05,
      "loss": 1.7181,
      "step": 91000
    },
    {
      "epoch": 0.36548981594409596,
      "grad_norm": 2.3991281986236572,
      "learning_rate": 3.172570783856474e-05,
      "loss": 1.7148,
      "step": 92000
    },
    {
      "epoch": 0.3694625313347926,
      "grad_norm": 2.5578675270080566,
      "learning_rate": 3.152707206902991e-05,
      "loss": 1.7213,
      "step": 93000
    },
    {
      "epoch": 0.37343524672548933,
      "grad_norm": 2.476649284362793,
      "learning_rate": 3.132843629949507e-05,
      "loss": 1.7162,
      "step": 94000
    },
    {
      "epoch": 0.37740796211618605,
      "grad_norm": 2.5002450942993164,
      "learning_rate": 3.112980052996023e-05,
      "loss": 1.7116,
      "step": 95000
    },
    {
      "epoch": 0.3813806775068827,
      "grad_norm": 2.4586169719696045,
      "learning_rate": 3.09311647604254e-05,
      "loss": 1.7094,
      "step": 96000
    },
    {
      "epoch": 0.3853533928975794,
      "grad_norm": 2.320626735687256,
      "learning_rate": 3.0732528990890564e-05,
      "loss": 1.7029,
      "step": 97000
    },
    {
      "epoch": 0.38932610828827613,
      "grad_norm": 2.5416829586029053,
      "learning_rate": 3.053389322135573e-05,
      "loss": 1.7074,
      "step": 98000
    },
    {
      "epoch": 0.39329882367897284,
      "grad_norm": 2.373359203338623,
      "learning_rate": 3.03352574518209e-05,
      "loss": 1.7053,
      "step": 99000
    },
    {
      "epoch": 0.3972715390696695,
      "grad_norm": 2.3184893131256104,
      "learning_rate": 3.013662168228606e-05,
      "loss": 1.7037,
      "step": 100000
    },
    {
      "epoch": 0.4012442544603662,
      "grad_norm": 2.4586610794067383,
      "learning_rate": 2.9937985912751228e-05,
      "loss": 1.6977,
      "step": 101000
    },
    {
      "epoch": 0.4052169698510629,
      "grad_norm": 2.2973012924194336,
      "learning_rate": 2.9739350143216394e-05,
      "loss": 1.6995,
      "step": 102000
    },
    {
      "epoch": 0.4091896852417596,
      "grad_norm": 2.6044728755950928,
      "learning_rate": 2.9540714373681556e-05,
      "loss": 1.6963,
      "step": 103000
    },
    {
      "epoch": 0.4131624006324563,
      "grad_norm": 2.183774948120117,
      "learning_rate": 2.9342078604146722e-05,
      "loss": 1.6937,
      "step": 104000
    },
    {
      "epoch": 0.417135116023153,
      "grad_norm": 2.3353705406188965,
      "learning_rate": 2.9143442834611888e-05,
      "loss": 1.7032,
      "step": 105000
    },
    {
      "epoch": 0.42110783141384966,
      "grad_norm": 2.4217607975006104,
      "learning_rate": 2.894480706507705e-05,
      "loss": 1.6971,
      "step": 106000
    },
    {
      "epoch": 0.4250805468045464,
      "grad_norm": 2.172797441482544,
      "learning_rate": 2.874617129554222e-05,
      "loss": 1.687,
      "step": 107000
    },
    {
      "epoch": 0.4290532621952431,
      "grad_norm": 2.1805503368377686,
      "learning_rate": 2.8547535526007385e-05,
      "loss": 1.6895,
      "step": 108000
    },
    {
      "epoch": 0.43302597758593975,
      "grad_norm": 2.3714077472686768,
      "learning_rate": 2.8348899756472548e-05,
      "loss": 1.6804,
      "step": 109000
    },
    {
      "epoch": 0.43699869297663646,
      "grad_norm": 2.608064651489258,
      "learning_rate": 2.8150263986937714e-05,
      "loss": 1.6821,
      "step": 110000
    },
    {
      "epoch": 0.4409714083673332,
      "grad_norm": 2.2420291900634766,
      "learning_rate": 2.795162821740288e-05,
      "loss": 1.6828,
      "step": 111000
    },
    {
      "epoch": 0.44494412375802983,
      "grad_norm": 2.4504592418670654,
      "learning_rate": 2.7752992447868042e-05,
      "loss": 1.6835,
      "step": 112000
    },
    {
      "epoch": 0.44891683914872654,
      "grad_norm": 2.330308198928833,
      "learning_rate": 2.7554356678333208e-05,
      "loss": 1.6801,
      "step": 113000
    },
    {
      "epoch": 0.45288955453942326,
      "grad_norm": 2.5981054306030273,
      "learning_rate": 2.7355720908798377e-05,
      "loss": 1.6735,
      "step": 114000
    },
    {
      "epoch": 0.4568622699301199,
      "grad_norm": 2.5059571266174316,
      "learning_rate": 2.715708513926354e-05,
      "loss": 1.6734,
      "step": 115000
    },
    {
      "epoch": 0.4608349853208166,
      "grad_norm": 2.427757740020752,
      "learning_rate": 2.6958449369728706e-05,
      "loss": 1.6735,
      "step": 116000
    },
    {
      "epoch": 0.46480770071151334,
      "grad_norm": 2.557253122329712,
      "learning_rate": 2.6759813600193868e-05,
      "loss": 1.6784,
      "step": 117000
    },
    {
      "epoch": 0.46878041610221,
      "grad_norm": 2.3222787380218506,
      "learning_rate": 2.6561177830659034e-05,
      "loss": 1.6705,
      "step": 118000
    },
    {
      "epoch": 0.4727531314929067,
      "grad_norm": 2.4362246990203857,
      "learning_rate": 2.63625420611242e-05,
      "loss": 1.6738,
      "step": 119000
    },
    {
      "epoch": 0.4767258468836034,
      "grad_norm": 2.5164308547973633,
      "learning_rate": 2.6163906291589363e-05,
      "loss": 1.6775,
      "step": 120000
    },
    {
      "epoch": 0.48069856227430013,
      "grad_norm": 2.529007911682129,
      "learning_rate": 2.596527052205453e-05,
      "loss": 1.6627,
      "step": 121000
    },
    {
      "epoch": 0.4846712776649968,
      "grad_norm": 2.4359118938446045,
      "learning_rate": 2.5766634752519698e-05,
      "loss": 1.6642,
      "step": 122000
    },
    {
      "epoch": 0.4886439930556935,
      "grad_norm": 2.3070337772369385,
      "learning_rate": 2.556799898298486e-05,
      "loss": 1.6632,
      "step": 123000
    },
    {
      "epoch": 0.4926167084463902,
      "grad_norm": 2.443965435028076,
      "learning_rate": 2.5369363213450026e-05,
      "loss": 1.6717,
      "step": 124000
    },
    {
      "epoch": 0.4965894238370869,
      "grad_norm": 2.233098030090332,
      "learning_rate": 2.5170727443915192e-05,
      "loss": 1.6611,
      "step": 125000
    },
    {
      "epoch": 0.5005621392277836,
      "grad_norm": 2.655993700027466,
      "learning_rate": 2.4972091674380358e-05,
      "loss": 1.6597,
      "step": 126000
    },
    {
      "epoch": 0.5045348546184802,
      "grad_norm": 2.6189663410186768,
      "learning_rate": 2.477345590484552e-05,
      "loss": 1.6579,
      "step": 127000
    },
    {
      "epoch": 0.508507570009177,
      "grad_norm": 2.4079437255859375,
      "learning_rate": 2.4574820135310686e-05,
      "loss": 1.6617,
      "step": 128000
    },
    {
      "epoch": 0.5124802853998737,
      "grad_norm": 2.2482354640960693,
      "learning_rate": 2.4376184365775852e-05,
      "loss": 1.6561,
      "step": 129000
    },
    {
      "epoch": 0.5164530007905703,
      "grad_norm": 2.263550043106079,
      "learning_rate": 2.4177548596241018e-05,
      "loss": 1.6533,
      "step": 130000
    },
    {
      "epoch": 0.5204257161812671,
      "grad_norm": 2.4337539672851562,
      "learning_rate": 2.397891282670618e-05,
      "loss": 1.653,
      "step": 131000
    },
    {
      "epoch": 0.5243984315719638,
      "grad_norm": 2.4516239166259766,
      "learning_rate": 2.378027705717135e-05,
      "loss": 1.6517,
      "step": 132000
    },
    {
      "epoch": 0.5283711469626604,
      "grad_norm": 2.541231870651245,
      "learning_rate": 2.3581641287636512e-05,
      "loss": 1.648,
      "step": 133000
    },
    {
      "epoch": 0.5323438623533572,
      "grad_norm": 2.5160932540893555,
      "learning_rate": 2.3383005518101678e-05,
      "loss": 1.6551,
      "step": 134000
    },
    {
      "epoch": 0.5363165777440538,
      "grad_norm": 2.3096656799316406,
      "learning_rate": 2.3184369748566844e-05,
      "loss": 1.6564,
      "step": 135000
    },
    {
      "epoch": 0.5402892931347505,
      "grad_norm": 2.33455228805542,
      "learning_rate": 2.298573397903201e-05,
      "loss": 1.6419,
      "step": 136000
    },
    {
      "epoch": 0.5442620085254473,
      "grad_norm": 2.5458242893218994,
      "learning_rate": 2.2787098209497172e-05,
      "loss": 1.6386,
      "step": 137000
    },
    {
      "epoch": 0.5482347239161439,
      "grad_norm": 2.476320743560791,
      "learning_rate": 2.258846243996234e-05,
      "loss": 1.6463,
      "step": 138000
    },
    {
      "epoch": 0.5522074393068406,
      "grad_norm": 2.2523152828216553,
      "learning_rate": 2.2389826670427504e-05,
      "loss": 1.6419,
      "step": 139000
    },
    {
      "epoch": 0.5561801546975373,
      "grad_norm": 2.379561424255371,
      "learning_rate": 2.219119090089267e-05,
      "loss": 1.6345,
      "step": 140000
    },
    {
      "epoch": 0.560152870088234,
      "grad_norm": 2.399991035461426,
      "learning_rate": 2.1992555131357836e-05,
      "loss": 1.6421,
      "step": 141000
    },
    {
      "epoch": 0.5641255854789307,
      "grad_norm": 2.470299243927002,
      "learning_rate": 2.1793919361823002e-05,
      "loss": 1.6409,
      "step": 142000
    },
    {
      "epoch": 0.5680983008696274,
      "grad_norm": 2.714812755584717,
      "learning_rate": 2.1595283592288164e-05,
      "loss": 1.6312,
      "step": 143000
    },
    {
      "epoch": 0.5720710162603241,
      "grad_norm": 2.2342469692230225,
      "learning_rate": 2.1396647822753334e-05,
      "loss": 1.6338,
      "step": 144000
    },
    {
      "epoch": 0.5760437316510207,
      "grad_norm": 2.4666154384613037,
      "learning_rate": 2.1198012053218496e-05,
      "loss": 1.6363,
      "step": 145000
    },
    {
      "epoch": 0.5800164470417175,
      "grad_norm": 2.2313907146453857,
      "learning_rate": 2.0999376283683662e-05,
      "loss": 1.6354,
      "step": 146000
    },
    {
      "epoch": 0.5839891624324142,
      "grad_norm": 2.390474319458008,
      "learning_rate": 2.0800740514148828e-05,
      "loss": 1.6386,
      "step": 147000
    },
    {
      "epoch": 0.5879618778231108,
      "grad_norm": 2.533613920211792,
      "learning_rate": 2.0602104744613994e-05,
      "loss": 1.628,
      "step": 148000
    },
    {
      "epoch": 0.5919345932138076,
      "grad_norm": 2.4117157459259033,
      "learning_rate": 2.0403468975079156e-05,
      "loss": 1.6304,
      "step": 149000
    },
    {
      "epoch": 0.5959073086045042,
      "grad_norm": 2.5512712001800537,
      "learning_rate": 2.0204833205544322e-05,
      "loss": 1.6315,
      "step": 150000
    },
    {
      "epoch": 0.5998800239952009,
      "grad_norm": 2.4903125762939453,
      "learning_rate": 2.0006197436009488e-05,
      "loss": 1.628,
      "step": 151000
    },
    {
      "epoch": 0.6038527393858977,
      "grad_norm": 2.2201976776123047,
      "learning_rate": 1.9807561666474654e-05,
      "loss": 1.6275,
      "step": 152000
    },
    {
      "epoch": 0.6078254547765943,
      "grad_norm": 2.547222375869751,
      "learning_rate": 1.9608925896939816e-05,
      "loss": 1.6286,
      "step": 153000
    },
    {
      "epoch": 0.6117981701672911,
      "grad_norm": 2.530766725540161,
      "learning_rate": 1.9410290127404986e-05,
      "loss": 1.6226,
      "step": 154000
    },
    {
      "epoch": 0.6157708855579878,
      "grad_norm": 2.4705374240875244,
      "learning_rate": 1.9211654357870148e-05,
      "loss": 1.6252,
      "step": 155000
    },
    {
      "epoch": 0.6197436009486844,
      "grad_norm": 2.3607566356658936,
      "learning_rate": 1.9013018588335314e-05,
      "loss": 1.6244,
      "step": 156000
    },
    {
      "epoch": 0.6237163163393812,
      "grad_norm": 2.2306785583496094,
      "learning_rate": 1.881438281880048e-05,
      "loss": 1.6266,
      "step": 157000
    },
    {
      "epoch": 0.6276890317300778,
      "grad_norm": 2.4895143508911133,
      "learning_rate": 1.8615747049265646e-05,
      "loss": 1.6159,
      "step": 158000
    },
    {
      "epoch": 0.6316617471207745,
      "grad_norm": 2.420661449432373,
      "learning_rate": 1.841711127973081e-05,
      "loss": 1.6232,
      "step": 159000
    },
    {
      "epoch": 0.6356344625114713,
      "grad_norm": 2.5235602855682373,
      "learning_rate": 1.8218475510195974e-05,
      "loss": 1.6218,
      "step": 160000
    },
    {
      "epoch": 0.6396071779021679,
      "grad_norm": 2.5885348320007324,
      "learning_rate": 1.801983974066114e-05,
      "loss": 1.6141,
      "step": 161000
    },
    {
      "epoch": 0.6435798932928646,
      "grad_norm": 2.399214267730713,
      "learning_rate": 1.7821203971126306e-05,
      "loss": 1.6234,
      "step": 162000
    },
    {
      "epoch": 0.6475526086835613,
      "grad_norm": 2.4853930473327637,
      "learning_rate": 1.7622568201591472e-05,
      "loss": 1.6184,
      "step": 163000
    },
    {
      "epoch": 0.651525324074258,
      "grad_norm": 2.3881783485412598,
      "learning_rate": 1.7423932432056634e-05,
      "loss": 1.6202,
      "step": 164000
    },
    {
      "epoch": 0.6554980394649547,
      "grad_norm": 2.6313488483428955,
      "learning_rate": 1.72252966625218e-05,
      "loss": 1.61,
      "step": 165000
    },
    {
      "epoch": 0.6594707548556514,
      "grad_norm": 2.24963116645813,
      "learning_rate": 1.7026660892986966e-05,
      "loss": 1.6141,
      "step": 166000
    },
    {
      "epoch": 0.6634434702463481,
      "grad_norm": 2.221066474914551,
      "learning_rate": 1.6828025123452132e-05,
      "loss": 1.6162,
      "step": 167000
    },
    {
      "epoch": 0.6674161856370447,
      "grad_norm": 2.4905431270599365,
      "learning_rate": 1.6629389353917298e-05,
      "loss": 1.611,
      "step": 168000
    },
    {
      "epoch": 0.6713889010277415,
      "grad_norm": 2.4833924770355225,
      "learning_rate": 1.6430753584382464e-05,
      "loss": 1.6156,
      "step": 169000
    },
    {
      "epoch": 0.6753616164184382,
      "grad_norm": 2.1998064517974854,
      "learning_rate": 1.6232117814847626e-05,
      "loss": 1.613,
      "step": 170000
    },
    {
      "epoch": 0.6793343318091348,
      "grad_norm": 2.6126694679260254,
      "learning_rate": 1.6033482045312792e-05,
      "loss": 1.6064,
      "step": 171000
    },
    {
      "epoch": 0.6833070471998316,
      "grad_norm": 2.442721128463745,
      "learning_rate": 1.5834846275777958e-05,
      "loss": 1.6081,
      "step": 172000
    },
    {
      "epoch": 0.6872797625905283,
      "grad_norm": 2.495753526687622,
      "learning_rate": 1.5636210506243124e-05,
      "loss": 1.6073,
      "step": 173000
    },
    {
      "epoch": 0.6912524779812249,
      "grad_norm": 2.303683042526245,
      "learning_rate": 1.5437574736708286e-05,
      "loss": 1.6052,
      "step": 174000
    },
    {
      "epoch": 0.6952251933719217,
      "grad_norm": 2.33467698097229,
      "learning_rate": 1.5238938967173452e-05,
      "loss": 1.6074,
      "step": 175000
    },
    {
      "epoch": 0.6991979087626183,
      "grad_norm": 2.382906436920166,
      "learning_rate": 1.504030319763862e-05,
      "loss": 1.6055,
      "step": 176000
    },
    {
      "epoch": 0.703170624153315,
      "grad_norm": 2.3838586807250977,
      "learning_rate": 1.4841667428103784e-05,
      "loss": 1.6049,
      "step": 177000
    },
    {
      "epoch": 0.7071433395440118,
      "grad_norm": 2.509345769882202,
      "learning_rate": 1.4643031658568948e-05,
      "loss": 1.6033,
      "step": 178000
    },
    {
      "epoch": 0.7111160549347084,
      "grad_norm": 2.452302932739258,
      "learning_rate": 1.4444395889034116e-05,
      "loss": 1.6014,
      "step": 179000
    },
    {
      "epoch": 0.7150887703254051,
      "grad_norm": 2.512484312057495,
      "learning_rate": 1.424576011949928e-05,
      "loss": 1.5952,
      "step": 180000
    },
    {
      "epoch": 0.7190614857161018,
      "grad_norm": 2.3485519886016846,
      "learning_rate": 1.4047124349964444e-05,
      "loss": 1.6028,
      "step": 181000
    },
    {
      "epoch": 0.7230342011067985,
      "grad_norm": 2.513551712036133,
      "learning_rate": 1.3848488580429612e-05,
      "loss": 1.6003,
      "step": 182000
    },
    {
      "epoch": 0.7270069164974952,
      "grad_norm": 2.660658597946167,
      "learning_rate": 1.3649852810894776e-05,
      "loss": 1.5975,
      "step": 183000
    },
    {
      "epoch": 0.7309796318881919,
      "grad_norm": 2.6458804607391357,
      "learning_rate": 1.345121704135994e-05,
      "loss": 1.601,
      "step": 184000
    },
    {
      "epoch": 0.7349523472788886,
      "grad_norm": 2.496195077896118,
      "learning_rate": 1.3252581271825106e-05,
      "loss": 1.5876,
      "step": 185000
    },
    {
      "epoch": 0.7389250626695852,
      "grad_norm": 2.7064778804779053,
      "learning_rate": 1.3053945502290272e-05,
      "loss": 1.6044,
      "step": 186000
    },
    {
      "epoch": 0.742897778060282,
      "grad_norm": 2.5149683952331543,
      "learning_rate": 1.2855309732755436e-05,
      "loss": 1.5883,
      "step": 187000
    },
    {
      "epoch": 0.7468704934509787,
      "grad_norm": 2.383103609085083,
      "learning_rate": 1.2656673963220602e-05,
      "loss": 1.6003,
      "step": 188000
    },
    {
      "epoch": 0.7508432088416753,
      "grad_norm": 2.451568603515625,
      "learning_rate": 1.2458038193685766e-05,
      "loss": 1.5917,
      "step": 189000
    },
    {
      "epoch": 0.7548159242323721,
      "grad_norm": 2.4144864082336426,
      "learning_rate": 1.2259402424150932e-05,
      "loss": 1.5993,
      "step": 190000
    },
    {
      "epoch": 0.7587886396230688,
      "grad_norm": 2.364407777786255,
      "learning_rate": 1.2060766654616096e-05,
      "loss": 1.5929,
      "step": 191000
    },
    {
      "epoch": 0.7627613550137654,
      "grad_norm": 2.4853672981262207,
      "learning_rate": 1.1862130885081262e-05,
      "loss": 1.5885,
      "step": 192000
    },
    {
      "epoch": 0.7667340704044622,
      "grad_norm": 2.654451370239258,
      "learning_rate": 1.1663495115546426e-05,
      "loss": 1.5758,
      "step": 193000
    },
    {
      "epoch": 0.7707067857951588,
      "grad_norm": 2.6151816844940186,
      "learning_rate": 1.1464859346011592e-05,
      "loss": 1.5878,
      "step": 194000
    },
    {
      "epoch": 0.7746795011858555,
      "grad_norm": 2.532261848449707,
      "learning_rate": 1.1266223576476758e-05,
      "loss": 1.5928,
      "step": 195000
    },
    {
      "epoch": 0.7786522165765523,
      "grad_norm": 2.3532674312591553,
      "learning_rate": 1.1067587806941922e-05,
      "loss": 1.5881,
      "step": 196000
    },
    {
      "epoch": 0.7826249319672489,
      "grad_norm": 2.4506990909576416,
      "learning_rate": 1.0868952037407088e-05,
      "loss": 1.5933,
      "step": 197000
    },
    {
      "epoch": 0.7865976473579457,
      "grad_norm": 2.415364980697632,
      "learning_rate": 1.0670316267872252e-05,
      "loss": 1.5814,
      "step": 198000
    },
    {
      "epoch": 0.7905703627486423,
      "grad_norm": 2.285956859588623,
      "learning_rate": 1.0471680498337418e-05,
      "loss": 1.5872,
      "step": 199000
    },
    {
      "epoch": 0.794543078139339,
      "grad_norm": 2.420119524002075,
      "learning_rate": 1.0273044728802584e-05,
      "loss": 1.5883,
      "step": 200000
    },
    {
      "epoch": 0.7985157935300358,
      "grad_norm": 2.477130889892578,
      "learning_rate": 1.0074408959267748e-05,
      "loss": 1.5837,
      "step": 201000
    },
    {
      "epoch": 0.8024885089207324,
      "grad_norm": 2.3316898345947266,
      "learning_rate": 9.875773189732914e-06,
      "loss": 1.5858,
      "step": 202000
    },
    {
      "epoch": 0.8064612243114291,
      "grad_norm": 2.4339334964752197,
      "learning_rate": 9.67713742019808e-06,
      "loss": 1.5929,
      "step": 203000
    },
    {
      "epoch": 0.8104339397021258,
      "grad_norm": 2.4766809940338135,
      "learning_rate": 9.478501650663244e-06,
      "loss": 1.589,
      "step": 204000
    },
    {
      "epoch": 0.8144066550928225,
      "grad_norm": 2.576587200164795,
      "learning_rate": 9.27986588112841e-06,
      "loss": 1.5853,
      "step": 205000
    },
    {
      "epoch": 0.8183793704835192,
      "grad_norm": 2.4619030952453613,
      "learning_rate": 9.081230111593576e-06,
      "loss": 1.5851,
      "step": 206000
    },
    {
      "epoch": 0.8223520858742159,
      "grad_norm": 2.5359294414520264,
      "learning_rate": 8.88259434205874e-06,
      "loss": 1.5756,
      "step": 207000
    },
    {
      "epoch": 0.8263248012649126,
      "grad_norm": 2.4730947017669678,
      "learning_rate": 8.683958572523906e-06,
      "loss": 1.5785,
      "step": 208000
    },
    {
      "epoch": 0.8302975166556092,
      "grad_norm": 2.442582130432129,
      "learning_rate": 8.48532280298907e-06,
      "loss": 1.5762,
      "step": 209000
    },
    {
      "epoch": 0.834270232046306,
      "grad_norm": 2.705512285232544,
      "learning_rate": 8.286687033454236e-06,
      "loss": 1.5762,
      "step": 210000
    },
    {
      "epoch": 0.8382429474370027,
      "grad_norm": 2.48193359375,
      "learning_rate": 8.088051263919402e-06,
      "loss": 1.574,
      "step": 211000
    },
    {
      "epoch": 0.8422156628276993,
      "grad_norm": 2.496122121810913,
      "learning_rate": 7.889415494384566e-06,
      "loss": 1.5813,
      "step": 212000
    },
    {
      "epoch": 0.8461883782183961,
      "grad_norm": 2.32611083984375,
      "learning_rate": 7.690779724849732e-06,
      "loss": 1.5781,
      "step": 213000
    },
    {
      "epoch": 0.8501610936090928,
      "grad_norm": 2.6972310543060303,
      "learning_rate": 7.492143955314898e-06,
      "loss": 1.5733,
      "step": 214000
    },
    {
      "epoch": 0.8541338089997894,
      "grad_norm": 2.5044896602630615,
      "learning_rate": 7.293508185780062e-06,
      "loss": 1.5747,
      "step": 215000
    },
    {
      "epoch": 0.8581065243904862,
      "grad_norm": 2.3337509632110596,
      "learning_rate": 7.094872416245228e-06,
      "loss": 1.5722,
      "step": 216000
    },
    {
      "epoch": 0.8620792397811828,
      "grad_norm": 2.544649839401245,
      "learning_rate": 6.896236646710394e-06,
      "loss": 1.5808,
      "step": 217000
    },
    {
      "epoch": 0.8660519551718795,
      "grad_norm": 2.4193694591522217,
      "learning_rate": 6.697600877175558e-06,
      "loss": 1.5799,
      "step": 218000
    },
    {
      "epoch": 0.8700246705625763,
      "grad_norm": 2.420445203781128,
      "learning_rate": 6.498965107640724e-06,
      "loss": 1.571,
      "step": 219000
    },
    {
      "epoch": 0.8739973859532729,
      "grad_norm": 2.541074514389038,
      "learning_rate": 6.300329338105888e-06,
      "loss": 1.5729,
      "step": 220000
    },
    {
      "epoch": 0.8779701013439696,
      "grad_norm": 2.393486976623535,
      "learning_rate": 6.101693568571054e-06,
      "loss": 1.5706,
      "step": 221000
    },
    {
      "epoch": 0.8819428167346663,
      "grad_norm": 2.468979835510254,
      "learning_rate": 5.90305779903622e-06,
      "loss": 1.5675,
      "step": 222000
    },
    {
      "epoch": 0.885915532125363,
      "grad_norm": 2.4232184886932373,
      "learning_rate": 5.704422029501385e-06,
      "loss": 1.5693,
      "step": 223000
    },
    {
      "epoch": 0.8898882475160597,
      "grad_norm": 2.5525453090667725,
      "learning_rate": 5.50578625996655e-06,
      "loss": 1.5669,
      "step": 224000
    },
    {
      "epoch": 0.8938609629067564,
      "grad_norm": 2.6029274463653564,
      "learning_rate": 5.307150490431715e-06,
      "loss": 1.5735,
      "step": 225000
    },
    {
      "epoch": 0.8978336782974531,
      "grad_norm": 2.4957664012908936,
      "learning_rate": 5.10851472089688e-06,
      "loss": 1.5668,
      "step": 226000
    },
    {
      "epoch": 0.9018063936881497,
      "grad_norm": 2.577777862548828,
      "learning_rate": 4.909878951362046e-06,
      "loss": 1.5666,
      "step": 227000
    },
    {
      "epoch": 0.9057791090788465,
      "grad_norm": 2.4812021255493164,
      "learning_rate": 4.711243181827211e-06,
      "loss": 1.5599,
      "step": 228000
    },
    {
      "epoch": 0.9097518244695432,
      "grad_norm": 3.040672779083252,
      "learning_rate": 4.512607412292376e-06,
      "loss": 1.562,
      "step": 229000
    },
    {
      "epoch": 0.9137245398602398,
      "grad_norm": 2.5048317909240723,
      "learning_rate": 4.313971642757541e-06,
      "loss": 1.5693,
      "step": 230000
    },
    {
      "epoch": 0.9176972552509366,
      "grad_norm": 2.626075267791748,
      "learning_rate": 4.115335873222706e-06,
      "loss": 1.5591,
      "step": 231000
    },
    {
      "epoch": 0.9216699706416333,
      "grad_norm": 2.540921449661255,
      "learning_rate": 3.916700103687871e-06,
      "loss": 1.565,
      "step": 232000
    },
    {
      "epoch": 0.9256426860323299,
      "grad_norm": 2.4145989418029785,
      "learning_rate": 3.7180643341530373e-06,
      "loss": 1.5623,
      "step": 233000
    },
    {
      "epoch": 0.9296154014230267,
      "grad_norm": 2.2476632595062256,
      "learning_rate": 3.5194285646182023e-06,
      "loss": 1.5706,
      "step": 234000
    },
    {
      "epoch": 0.9335881168137233,
      "grad_norm": 2.725071668624878,
      "learning_rate": 3.3207927950833674e-06,
      "loss": 1.5687,
      "step": 235000
    },
    {
      "epoch": 0.93756083220442,
      "grad_norm": 2.4635486602783203,
      "learning_rate": 3.122157025548533e-06,
      "loss": 1.5663,
      "step": 236000
    },
    {
      "epoch": 0.9415335475951168,
      "grad_norm": 2.4632797241210938,
      "learning_rate": 2.923521256013698e-06,
      "loss": 1.5659,
      "step": 237000
    },
    {
      "epoch": 0.9455062629858134,
      "grad_norm": 2.292675018310547,
      "learning_rate": 2.7248854864788633e-06,
      "loss": 1.559,
      "step": 238000
    },
    {
      "epoch": 0.9494789783765101,
      "grad_norm": 2.7259786128997803,
      "learning_rate": 2.5262497169440284e-06,
      "loss": 1.5561,
      "step": 239000
    },
    {
      "epoch": 0.9534516937672068,
      "grad_norm": 2.3121955394744873,
      "learning_rate": 2.327613947409194e-06,
      "loss": 1.5607,
      "step": 240000
    },
    {
      "epoch": 0.9574244091579035,
      "grad_norm": 2.587742567062378,
      "learning_rate": 2.128978177874359e-06,
      "loss": 1.564,
      "step": 241000
    },
    {
      "epoch": 0.9613971245486003,
      "grad_norm": 2.39577579498291,
      "learning_rate": 1.9303424083395243e-06,
      "loss": 1.5585,
      "step": 242000
    },
    {
      "epoch": 0.9653698399392969,
      "grad_norm": 2.9247663021087646,
      "learning_rate": 1.7317066388046894e-06,
      "loss": 1.5655,
      "step": 243000
    },
    {
      "epoch": 0.9693425553299936,
      "grad_norm": 2.632439374923706,
      "learning_rate": 1.5330708692698546e-06,
      "loss": 1.5596,
      "step": 244000
    },
    {
      "epoch": 0.9733152707206904,
      "grad_norm": 2.566711187362671,
      "learning_rate": 1.3344350997350199e-06,
      "loss": 1.5599,
      "step": 245000
    },
    {
      "epoch": 0.977287986111387,
      "grad_norm": 2.4932680130004883,
      "learning_rate": 1.1357993302001851e-06,
      "loss": 1.5624,
      "step": 246000
    },
    {
      "epoch": 0.9812607015020837,
      "grad_norm": 2.420874834060669,
      "learning_rate": 9.371635606653504e-07,
      "loss": 1.5575,
      "step": 247000
    },
    {
      "epoch": 0.9852334168927804,
      "grad_norm": 2.635591745376587,
      "learning_rate": 7.385277911305156e-07,
      "loss": 1.5573,
      "step": 248000
    },
    {
      "epoch": 0.9892061322834771,
      "grad_norm": 2.9403605461120605,
      "learning_rate": 5.398920215956809e-07,
      "loss": 1.5642,
      "step": 249000
    },
    {
      "epoch": 0.9931788476741737,
      "grad_norm": 2.488065004348755,
      "learning_rate": 3.412562520608461e-07,
      "loss": 1.5681,
      "step": 250000
    },
    {
      "epoch": 0.9971515630648705,
      "grad_norm": 2.475478410720825,
      "learning_rate": 1.4262048252601137e-07,
      "loss": 1.551,
      "step": 251000
    }
  ],
  "logging_steps": 1000,
  "max_steps": 251717,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.718572361095578e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
